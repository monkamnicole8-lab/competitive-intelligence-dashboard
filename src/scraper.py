"""
Module de collecte de donn√©es depuis des APIs et sites web
"""

import requests
import json
import pandas as pd
from datetime import datetime
import time
import sys
import os

# Ajouter les dossiers au path Python
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, current_dir)

from src.logger import get_logger, load_config

# Cr√©er le logger
logger = get_logger(__name__)


def test_api_connection(config):
    """
    Teste la connexion √† une API publique
    """
    base_url = config['api']['base_url']
    endpoint = config['api']['endpoints']['products']
    url = f"{base_url}{endpoint}"
    timeout = config['api']['timeout']
    
    logger.info(f"Envoi de la requ√™te √† l'API : {url}")
    
    try:
        response = requests.get(url, timeout=timeout)
        
        if response.status_code == 200:
            logger.info(f"‚úÖ Connexion r√©ussie (code {response.status_code})")
            data = response.json()
            logger.info(f"üìä {len(data)} produits r√©cup√©r√©s")
            return data
        else:
            logger.error(f"‚ùå Erreur HTTP : code {response.status_code}")
            return None
            
    except requests.exceptions.Timeout:
        logger.error(f"‚è±Ô∏è  Timeout apr√®s {timeout} secondes")
        return None
    except requests.exceptions.ConnectionError:
        logger.error("üîå Erreur de connexion r√©seau")
        return None
    except Exception as e:
        logger.critical(f"‚ùå Erreur inattendue : {e}", exc_info=True)
        return None


def save_products_to_csv(products, config, filename=None):
    """
    Transforme les donn√©es JSON en DataFrame et sauvegarde en CSV
    """
    if not products:
        logger.warning("‚ö†Ô∏è  Aucune donn√©e √† sauvegarder")
        return None
    
    logger.info("üîÑ Transformation des donn√©es en DataFrame...")
    
    df = pd.DataFrame(products)
    df = df[['id', 'title', 'price', 'category']]
    
    logger.info(f"üìä DataFrame cr√©√© : {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
    
    if filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{config['paths']['raw_data']}/products_{timestamp}.csv"
    
    df.to_csv(filename, index=False, encoding='utf-8')
    logger.info(f"‚úÖ Donn√©es sauvegard√©es : {filename}")
    
    return df


def run_scraper(config):
    """
    Ex√©cute le pipeline complet de collecte
    """
    logger.info("=" * 60)
    logger.info("üöÄ D√âMARRAGE DU SCRAPER")
    logger.info("=" * 60)
    
    start_time = time.time()
    
    products = test_api_connection(config)
    
    if not products:
        logger.error("‚ùå √âchec de la collecte de donn√©es")
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{config['paths']['raw_data']}/products_{timestamp}.csv"
    df = save_products_to_csv(products, config, filename)
    
    elapsed_time = time.time() - start_time
    logger.info("=" * 60)
    logger.info(f"‚úÖ SCRAPER TERMIN√â en {elapsed_time:.2f} secondes")
    logger.info(f"üìÇ Fichier cr√©√© : {filename}")
    logger.info("=" * 60)
    
    return filename


if __name__ == "__main__":
    config = load_config()
    run_scraper(config)